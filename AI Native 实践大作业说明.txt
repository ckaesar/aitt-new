AINative实践⼤作业说明

📋⼀、总体设计

1.1核⼼⽬标

通过⼀个完整的AI应⽤开发项⽬,让团队成员掌握:

• ✅⼤模型API调⽤与参数调优

• ✅Prompt⼯程设计与迭代优化

• ✅RAG或Agent开发实践

• ✅量化评测与效果调优

• ✅AICoding⼯具的深度使⽤

1.2设计原则

原则

⻆⾊公平

说明

前端、后端、测试都需完成前后端开发,打破⻆⾊

边界

技能全覆盖

必须涉及⼤模型、Prompt、RAG/Agent、评测

四⼤模块

实操优先

做出能跑、能⽤、能部署的真实应⽤

AICoding

⿎励使⽤AI⼯具辅助开发,但需记录过程

可评测

标准化提交,便于快速review和防抄袭

1.3作业形式

-个⼈或⼩组:允许1-3⼈组队

-时间周期:6周(10⽉13⽇→11⽉21⽇)

-场景选择:完全⾃定义,结合真实业务需求

-难度定位:类似毕业设计,聚焦AI能⼒⽽⾮⼯程复杂度

🎯⼆、场景要求

2.1⾃定义场景规则

完全开放选题,但必须满⾜以下条件:

核⼼技术要求

• ✅调⽤⾄少1个⼤模型API(Claude/GPT/国产模型均可)

• ✅必须有Prompt设计与优化(⾄少3轮迭代)

• ✅必须实现RAG或Agent之⼀(或两者都有)

• ✅必须有量化评测体系(⾄少30条测试⽤例)

功能要求

• ✅有可交互的Web前端界⾯(不能只是命令⾏)

• ✅有后端API服务(⾄少3个接⼝)

• ✅能够部署到服务器并提供访问地址

• ✅解决⼀个真实的业务或学习场景问题

📦三、参考案例:项⽬⽂档智能问答系统

这是⼀个技术参考案例,仅供没有想法的同学参考,⿎励完全⾃定义其他场景

3.1案例背景与⽬标

场景描述:

开发⼀个智能问答系统,帮助团队成员快速查询内部技术⽂档(API⽂档、架构设计、操作⼿册等)。传统

的全⽂搜索经常找不到答案或结果过多,需要⽤RAG技术实现智能语义检索和精准回答。

核⼼功能:

1.⽂档管理:上传PDF/Markdown/Word⽂档,⾃动解析和向量化

2.智能问答:基于RAG检索相关内容,⽤⼤模型⽣成答案并标注来源

3.持续优化:通过Prompt迭代和评测体系不断提升准确率

4.Web界⾯:提供友好的⽤⼾界⾯,⽀持⽂件上传和对话交互

3.2技术实现思路

1.⽂档处理流程

代码块

1

2

3

4

⽂档上传 → 格式解析(PyPDF2/python-docx)
        → ⽂本分⽚(按段落/固定⻓度,overlap=50字)
        → 向量化(OpenAI embedding/本地模型)
        → 存储(Chroma/FAISS向量数据库)

关键点:

• 分⽚策略影响检索质量:段落级(适合结构化⽂档)vs固定⻓度(适合⻓⽂本)

• 需要保存原始⽂档信息,⽤于答案溯源

2.RAG检索实现

代码块

1

2

3

4

5

6

7

8

9

10

11

12

# 检索⽰意代码 
def rag_search(question, top_k=5):
    # 1. 问题向量化 
    q_vector = embedding_model.encode(question)

    # 2. 向量检索 
    results = vector_db.search(q_vector, top_k=top_k)

    # 3. 构建上下⽂ 
    context = "\n\n".join([
        f"[⽂档{i}:{r.doc_name}-段{r.chunk_id}]\n{r.text}"
        for i, r in enumerate(results)

    ])

    return context, results

13

14

15

关键点:

• top-k参数影响召回率和精准度(建议3-7)

• 可以尝试混合检索:向量检索(语义相似)+BM25(关键词匹配)

3.Prompt⼯程⽰例

V1版本(基础)

代码块

1

2

3

4

5

6

7

8

你是⼀个技术⽂档助⼿,根据以下⽂档⽚段回答问题。

⽂档内容:
{context}

⽤⼾问题: {query}

请回答:

问题:*经常编造内容,不标注来源,准确率只有65%

V2版本(添加约束)

代码块

1

2

3

4

5

6

7

8

9

10

11

你是技术⽂档问答助⼿,请严格基于给定⽂档回答问题。

规则:
1. 仅使⽤下⽅⽂档内容,不要编造
2. 如果⽂档中没有相关信息,回答"⽂档中未找到相关内容"
3. 回答简洁,不超过200字

⽂档内容:
{context}

⽤⼾问题: {query}

改进:幻觉减少,准确率提升到72%,但仍不标来源

V3版本(格式化输出)

代码块

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

你是技术⽂档问答助⼿,请严格基于给定⽂档回答问题,并标注来源。

回答格式:
答案: [你的回答,不超过150字]
来源: [⽂档X-段Y], [⽂档Z-段W]

规则:
- 严格使⽤⽂档内容,禁⽌编造
- 必须标注每条信息的来源⽚段
- 如⽆相关内容,回答"未找到"

---
⽂档内容:
{context}

⽤⼾问题: {query}

效果:准确率78%,引⽤标注率95%✅

迭代总结:

• V1→V2:加约束减少幻觉(+7%)

• V2→V3:强制格式化输出提升溯源性(+6%)

• 关键:每次改动都要⽤测试集量化效果

4.评测体系设计

测试数据集构建(30条样本)

代码块

1

2

3

4

5

6

7

8

9

10

11

12

13

14

{

  "test_cases": [

    {

      "id": "Q001",
      "question": "系统⽀持哪些⽂件格式?",
      "expected_keywords": ["PDF", "Markdown", "Word"],

      "difficulty": "easy"

    },

    {

      "id": "Q015",
      "question": "如何优化检索性能?",
      "expected_keywords": ["缓存", "索引", "分⽚"],
      "difficulty": "medium"

    },

15

16

17

18

19

20

21

22

23

    {

      "id": "Q030",
      "question": "向量数据库的索引算法对⽐?",
      "expected_keywords": ["HNSW", "IVF", "准确率", "速度"],
      "difficulty": "hard"

    }
    // ... 总共⾄少30条 
  ]

}

⾃动化评测脚本(evaluate.py)

代码块

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

def evaluate_system():

    results = []

    for case in test_cases:
        # 1. 调⽤问答系统 
        answer = qa_system.ask(case["question"])

        # 2. 检查关键词覆盖 
        accuracy = check_keywords(answer, case["expected_keywords"])

        # 3. 统计延迟和成本 
        latency = answer.metadata["latency_ms"]

        tokens = answer.metadata["total_tokens"]

        results.append({

            "id": case["id"],

            "accuracy": accuracy,

            "latency": latency,

            "tokens": tokens

        })

    # 输出量化指标 
    return {

        "accuracy": avg([r["accuracy"] for r in results]),

        "avg_latency_ms": avg([r["latency"] for r in results]),

        "avg_tokens": avg([r["tokens"] for r in results]),

        "cost_per_query": calculate_cost(avg_tokens)

    }

输出⽰例(metrics.json)

代码块

1

2

3

4

5

6

7

{

  "accuracy": 0.78,

  "avg_latency_ms": 1250,

  "avg_tokens": 450,

  "cost_per_query": 0.02,

  "citation_accuracy": 0.95

}

3.3案例总结

这个案例涵盖了所有技术要求:

• ✅⼤模型应⽤:Claude/GPTAPI调⽤,Prompt三轮迭代

• ✅RAG实现:向量检索+引⽤标注

• ✅评测体系:30条测试⽤例,⾃动化脚本

• ✅前后端:Web界⾯+RESTfulAPI

• ✅可部署:提供访问地址

✅四、硬性技术要求(100分基础分)

4.1基础架构(20分)

要求项

前端界⾯

具体标准

评分

必须有可交互的Web界⾯,⾄

8分

少包含:输⼊区、输出区、状

态展⽰

后端API

RESTfulAPI,⾄少包含3个接

6分

⼝(如 /chat ,
/evaluate , /health )

部署要求

4分

必须部署到虚拟机/服务器,提

供可访问的URL

⼀键启动

提供启动脚本,评审⼈配

2分

置 .env 后可⼀键启动

注意事项:

-⚠后端禁⽌使⽤Java(⿎励Python/Node.js/Go等,增加挑战)

• ⚠不能只做命令⾏⼯具,必须有Web界⾯

• ⚠部署地址必须在提交时可访问

4.2⼤模型应⽤(25分)

要求项

模型调⽤

具体标准

⾄少对接1个⼤模型

API(Claude/GPT/国产模型均

可)

评分

5分

Prompt⼯程

展⽰⾄少3轮Prompt迭代,记

10分

录每版提⽰词与效果对⽐

参数调优

对⽐不同

5分

temperature/top_p/max_to

kens对结果的影响

异常处理

超时重试、错误降级(如切换

5分

模型或默认回复)

关键要求:

-✅Prompt迭代必须有量化数据⽀撑,不能只说"感觉更好"

• ✅每次迭代要说明:改了什么、为什么改、效果如何

• ✅参数对⽐⾄少测试2组不同配置

4.3RAG或Agent实现(30分,⼆选⼀或都做)

选项A:RAG实现

要求项

⽂档处理

具体标准

评分

⽀持多格式⽂档,实现⽂档解

10分

析与分⽚

向量检索

实现向量检索

10分

(embedding+向量数据库),返

回相关⽚段

引⽤标注

答案必须标注来源,可追溯到

5分

原始⽂档⽚段

知识管理

⽀持动态添加/删除知识库内

5分

容

选项B:Agent实现

要求项

⼯具定义

具体标准

评分

⾄少实现2个可调⽤⼯具(如搜索、计算、⽂件操

5分

作)

决策链路

显⽰Agent思考过程

10分

(observation→thought→action)

失败重试

多步协作

⼯具调⽤失败时的⾃动恢复机制

5分

⾄少有1个任务需要调⽤2次以上⼯具完成

10分

4.4评测与优化(20分)

要求项

具体标准

评分

测试数据集

准备⾄少30条测试⽤例(问题+标准答案/预期

5分

⾏为)

⾃动化评测

提供可独⽴运⾏的评测脚本,输出量化指标

5分

量化指标

⾄少包含3类:准确性、性能(延迟)、成本

5分

(token)

迭代证明

展⽰⾄少2轮优化,每轮有"改动→指标变化"对

5分

⽐

评测指标⽰例:

代码块

1

2

3

4

5

6

7

{
  "accuracy": 0.78,           // 准确性 
  "avg_latency_ms": 1250,     // 平均延迟 
  "avg_tokens": 450,          // 平均token消耗 
  "cost_per_query": 0.02,     // 单次查询成本 
  "citation_accuracy": 0.85   // 引⽤准确率(如果做RAG) 
}

4.5⼯程质量(5分)

要求项

代码规范

具体标准

评分

关键函数有注释,变量命名清

1分

晰

⽇志记录

记录每次请求的模型、

2分

token、耗时、检索⽚段ID

安全防护

Prompt注⼊防护、敏感信息

1分

脱敏、输⼊校验

Git提交

⾄少10次渐进式提交,不能⼀

1分

次性提交完成

🌟五、加分项(最⾼+30分)

5.1创新与优化类

加分项

要求

分值

⾃动化Prompt调优

实现A/B测试或参数⽹格搜索,

+5分

有量化数据对⽐

多模型对⽐融合

对⽐≥2个模型并智能选择/融

+5分

合结果

Agent多步协作

实现复杂任务的多步⼯具调⽤

+5分

(≥3步),展⽰决策链路

RAG⾼级优化

混合检索(向量+BM25)、重排

+5分

序、查询改写等⾼级技术

5.2业务价值类

加分项

要求

分值

实际上线使⽤

在团队内真实使⽤≥2周,收集

+10分

≥20条⽤⼾反馈,并根据反馈

迭代优化(需提供反馈记录和

改进说明)

5.3AICoding沉淀类

加分项

要求

分值

深度使⽤记录

ai_usage.md详实记录AI使⽤

+5分

过程(>1500字,≥3个具体案

例,含提效数据)

Prompt模式库

总结≥3个可复⽤的Prompt

+5分

设计模式,每个有效果验证数

据

重要说明:

• ⚠加分项需提供充分证据(代码、数据、截图、视频)

• ⚠不能只写⽂档说明,必须有实际实现

• ⚠UI美观不作为主要评分依据,避免前端⻆⾊优势

• ⚠AICoding深度实践的评判标准⻅第六章ai_usage.md模板

📦六、提交要求

6.1必须提交的材料

序号

材料名称

说明

1

2

3

4

5

6

Git仓库地址

含渐进式提交历史(⾄少10次

commit)

部署访问地址

虚拟机IP:端⼝或在线服务

URL

README.md

说明启动⽅式、技术架构、核

⼼功能

评测脚本

可独⽴运⾏,输出量化指标

Demo视频

3-5分钟,演⽰功能与优化对⽐

ai_usage.md

AI使⽤记录(防抄袭关键⽂档)

6.2⽬录组织(建议,⾮强制)

代码块

1

2

3

4

5

6

你的项⽬/
├── (⾃定义的代码结构) 
├── README.md              # 必须,说明如何启动和使⽤ 
├── ai_usage.md            # 必须,记录AI使⽤过程 
├── evaluate.sh            # 必须,评测脚本(可以是其他名字) 
├── .env.example           # 建议,⽅便评审⼈配置 

7

8

└── data/                  # 建议,存放测试数据 
    └── test_cases.json 

灵活性说明:

• ✅代码⽬录结构完全⾃定义

• ✅评测脚本名字⾃定义(但需在README中说明)

• ✅适配不同技术栈和项⽬组织⽅式

6.3核⼼⽂档要求

README.md模板

代码块

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

# 项⽬名称 

## 项⽬简介 
(1-2段话说明做什么、解决什么问题)

## 快速开始 

### 环境要求 
- Node.js 18+ / Python 3.9+
- Docker (可选)

### 启动步骤 
1. 安装依赖: `npm install` 或 `pip install -r requirements.txt`
2. 配置环境: 复制`.env.example`为`.env`并填写API密钥
3. 启动服务: `npm start` 或 `python main.py`
4. 访问地址: http://localhost:3000

### 评测运⾏ 
bash evaluate.sh

## 技术架构 
(贴上架构图或⽂字描述)

## 核⼼功能 
- [x] 功能1: ⼤模型调⽤
- [x] 功能2: RAG检索
- [x] 功能3: 评测系统
- [ ] 功能4: (未完成的)

29

30

31

32

33

34

35

36

37

38

39

40

## 性能指标 
| 指标 | 数值 |
|------|------|
| 准确率 | 78% |
| 平均延迟 | 1.2s |
| Token消耗 | 450/次 |

## 团队分⼯(如果是⼩组) 
- 张三: 前端开发、UI设计
- 李四: 后端架构、RAG实现
- 王五: 评测体系、⽂档编写

ai_usage.md模板(重要!)

代码块

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

# AI⼯具使⽤记录 

> 这是防抄袭的关键⽂档,请详细记录AI使⽤过程 

## 使⽤的AI⼯具 
- Claude Code: 60%代码⽣成
- ChatGPT: Prompt设计与调试
- Cursor: 代码补全

## 关键代码⽚段说明 

### 1. [模块名称,如"RAG检索模块"] 
**AI⽣成占⽐:** 约70% 

**使⽤的Prompt:** 
```
实现⼀个语义检索函数,使⽤sentence-transformers⽣成向量,
faiss进⾏检索,⽀持top-k参数...
```

**AI产出效果:** 
⽣成了基础框架,但检索策略需要⼿动优化

**⼿动修改部分:** 
- 添加了BM25混合检索
- 调整了相似度阈值从0.7到0.8
- 修复了内存泄漏问题

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

---

### 2. [另⼀个模块]
...

## Prompt迭代过程
(粘贴Prompt优化的3个版本及效果对⽐)

## 时间与效率提升
- 传统开发预计: 80⼩时
- 使⽤AI后实际: 35⼩时
- 效率提升: 56%

## 遇到的问题与解决
1. AI⽣成的代码有内存泄漏 → ⼈⼯review修复
2. Prompt效果不稳定 → 通过评测数据反复调优
3. ...

6.4提交⽅式

在指定的⻜书/腾讯⽂档表格中提交:

字段

项⽬名称

Git仓库

部署地址

说明

你的项⽬名称

GitHub/GitLab地址

http://x.x.x.x:xxxx

Demo视频

视频链接(腾讯视频/B站)

团队成员

提交时间

姓名+⼯号(如果是⼩组)

⾃动记录

提交截⽌:11⽉21⽇23:59

⚠提交后不可修改,请确保所有链接可访问

🛡七、防抄袭与作弊机制

7.1过程留痕检查

检查项

检查⽅式

判定标准

Git提交历史

查看commit记录

⾄少10次渐进式提交,⼀次性

提交扣20分

AI使⽤记录

审查ai_usage.md

必须详细具体,空洞或过于简

单扣10分

代码注释

抽查关键函数

缺少中⽂注释说明扣5分

提交时间分布

查看commit时间

集中在最后1-2天扣10分

7.2个性化要求(防完全抄袭)

每⼈提交作业时需在评测输出中包含以下信息:

代码块

1

2

3

4

5

{
  "submitter_id": "zhangsan_1234",  // 提交⼈ID(姓名_⼯号后4位) 
  "test_data_theme": "⻉壳找房FAQ", // 测试数据主题 
  "prompt_signature": "1234"        // ⼯号后4位(嵌⼊Prompt中) 
}

个性化设置说明:

• 在系统Prompt中加⼊⼯号后4位(如"你是编号1234的助⼿...")

• 测试数据集选择个性化主题(不能完全相同)

• 评测脚本输出必须包含提交⼈ID

7.3⼈⼯抽查机制

重点审查项

评审时重点检查以下内容(不需要专⻔⼯具):

ai_usage.md真实性检查

• 是否详实具体,有代码⽚段说明

• 是否有实际遇到的问题与解决过程

• 语⾔是否真实(不是AI⽣成的空话)

Git提交历史检查

• 是否⾄少10次提交

• 提交是否渐进式(不是⼀次性)

• 提交时间是否分散(不是最后集中)

代码与⽂档⼀致性

• ai_usage.md描述是否与实际代码匹配

• Prompt迭代是否有真实的版本记录

• 评测数据是否真实可信

现场答辩(抽查部分作业)

如果怀疑抄袭或造假,安排10分钟答辩:

1.代码讲解(5分钟)

• "请解释你的XX模块实现思路"

• "为什么这样设计?"

2.现场⼩改动(5分钟)

• "把top-k从5改到10,重新跑评测"

• "在Prompt中加⼀条新约束"

• 看是否能快速定位并修改

答辩判断:

• 完全答不上来→怀疑抄袭,扣30-50分

• 磕磕绊绊但能说清→可信,不扣分

• 流畅清晰→真实完成

7.4合作规范

✅允许的合作

• 讨论技术⽅案与实现思路

• 分享公开的技术⽂档和教程

• 互相codereview提建议

• 讨论Prompt设计⽅法(但不能复制)

❌严禁的⾏为

• 直接复制他⼈代码(超过20⾏连续相同)

• 共享核⼼Prompt模板(可讨论但不能照抄)

• 提交相同的评测数据集

• ⼀⼈完成多⼈署名

组队说明(2-3⼈)

• 必须在README.md中明确分⼯

• 每⼈必须独⽴完成⾄少1个核⼼模块

• 评审时会单独询问每个成员的负责部分

• 组队作品仍需满⾜所有技术要求

7.5组队分差机制

针对组队项⽬,将项⽬整体得分按贡献度分配给各成员,避免"⼤锅饭"

评分公式

个⼈最终得分=项⽬得分×个⼈贡献系数

说明:

-项⽬得分:按第四、五章节标准评定(100基础分+最⾼30加分)

-贡献系数:根据组内排名确定,⽤于将项⽬分分配给个⼈

贡献系数设定

组内排名

贡献系数

适⽤情况

第1名

0.95-1.00

主要贡献者,负责核⼼模块(如

RAG引擎、Prompt优化、评

测体系)

第2名

第3名

系数约束:

0.85-0.95

均衡贡献者,负责重要模块(如

前后端开发、API设计)

0.70-0.85

辅助贡献者,负责⽀撑⼯作(如

部署、⽂档、UI美化)

• 组内第1名最⾼系数为1.0(不能超过单⼈独⽴完成)

• 组内所有成员系数总和≈组内⼈数×0.85-0.90(体现协作分⼯的成本)

⽰例场景

场景1:3⼈组队,项⽬得分85分

-张三(第1名,系数1.00):85×1.00=85.0分

-李四(第2名,系数0.90):85×0.90=76.5分

-王五(第3名,系数0.75):85×0.75=63.8分

• 系数总和:1.00+0.90+0.75=2.65(3⼈×0.88)

场景2:2⼈组队,项⽬得分100分(满分)

-张三(第1名,系数1.00):100×1.00=100分

-李四(第2名,系数0.80):100×0.80=80分

• 系数总和:1.00+0.80=1.80(2⼈×0.90)

场景3:3⼈组队,项⽬得分120分(含加分项)

-张三(第1名,系数1.00):120×1.00=120分

-李四(第2名,系数0.88):120×0.88=105.6分

-王五(第3名,系数0.75):120×0.75=90分

对⽐单⼈:

-单⼈拿同样120分项⽬:直接得120分

• 组队第1名:最⾼120分(与单⼈持平)

• 组队第2/3名:明显低于单⼈(105分/90分)

最⼤分差:组内最⾼与最低可相差约30-35分(视项⽬得分和贡献差异)

排名依据

依据

Git提交数据

模块复杂度

答辩表现

提交要求

权重

40%

35%

25%

统计⽅式

提交次数、有效代码⾏数、核

⼼⽂件修改

核⼼技术模块(⾼)>前后端框

架(中)>⽂档部署(低)

能否流畅讲解负责模块,现场

改动熟练度

在README.md中添加"团队分⼯与贡献度"章节:

代码块

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

## 团队分⼯与贡献度 

### 成员分⼯ 
| 姓名 | 负责模块 | 建议系数 |
|------|---------|---------|
| 张三 | RAG检索引擎、Prompt优化、评测体系 | 1.0 |
| 李四 | 前端界⾯、后端API | 0.95 |
| 王五 | ⽂档编写、部署配置 | 0.85 |

### Git贡献统计 
- 张三: 45次提交, 1200⾏代码(核⼼逻辑)
- 李四: 30次提交, 800⾏代码(前后端)
- 王五: 10次提交, 300⾏代码(⽂档+配置)

### 组内共识 
全体成员已讨论并认可此分⼯,⽆异议。

📊⼋、评审流程与评分标准

8.1快速评审流程(每份15分钟)

第1步:环境启动(3分钟)

代码块

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

# 1. 克隆仓库 
git clone <repo_url>

cd <project_dir>

# 2. 检查Git提交历史 
git log --oneline | head -20

# 3. 配置环境变量 
cp .env.example .env
# 填⼊统⼀的测试密钥 

# 4. 启动服务 
bash start.sh  # 或按README说明启动 

# 5. 检查服务状态 
curl http://localhost:3000/health

检查点:

• ✅能否⼀键启动(不能则扣10分)

• ✅Git提交是否渐进(少于10次扣分)

第2步:功能验证(5分钟)

• 打开前端界⾯,测试基础功能

• 查看是否有引⽤标注/⼯具调⽤轨迹

• 尝试⼀个失败案例(如⽆关问题)

• 检查错误处理是否优雅

检查点:

• ✅功能是否完整

• ✅交互是否流畅

• ✅异常处理是否合理

第3步:评测验证(3分钟)

代码块# 运⾏评测脚本 
1
2

bash evaluate.sh

3

4

5

# 查看输出指标 
cat metrics.json

检查点:

• ✅评测脚本能否正常运⾏

• ✅指标是否达到基线(准确率≥60%,延迟≤5s)

• ✅是否有2轮以上优化对⽐

第4步:⽂档检查(4分钟)

• 阅读ai_usage.md是否详实

• 查看Prompt迭代过程

• 检查架构图是否清晰

• 观看Demo视频(快进查看重点)

检查点:

• ✅ai_usage.md是否真实详细

• ✅Prompt迭代是否有数据⽀撑

• ✅⽂档是否规范完整

8.2评分表

维度

基础架构

⼤模型应⽤

RAG/Agent

评测优化

满分

20

25

25

20

评分要点

前后端完整8分、部署4分、

API6分、启动2分

Prompt迭代10分、模型调⽤

5分、参数调优5分、异常处

理5分

向量检索10分、引⽤标注5

分、⽂档处理5分、知识管理

5分

测试集5分、⾃动化5分、量

化指标5分、迭代证明5分

代码规范2分、⽇志3分、安

全3分、Git提交2分

基础分

创新优化、⼯程深度、安全对

抗、业务价值

⽆法启动-10、抄袭-30、提交

不规范-10

最⾼130分(100基础+30加分-

扣分)

占⽐

20%

60%

15%

5%

说明

技术全⾯,有创新点,⼯程

质量⾼

满⾜所有基础要求,部分有

亮点

基本满⾜要求,但有明显不

⾜

重要功能缺失或⽆法运⾏,

需补交

⼯程质量

⼩计

加分项

扣分项

总分

8.3分档标准

等级

优秀

良好

及格

10

100

+30

-50

130

分数区间

≥90分

70-89分

60-69分

不及格

<60分

优秀作品特征:

• ✅所有基础要求都完成得很好

• ✅⾄少有2-3个加分项

• ✅⼯程质量⾼,⽂档规范

• ✅有创新或深度思考

不及格情况:

• ❌⽆法启动或功能严重缺失

• ❌没有评测体系

• ❌Prompt迭代缺失或造假

• ❌明显抄袭

💡九、常⻅问题FAQ

Q1:必须使⽤特定的技术栈吗?

A:不必须。后端禁⽌Java(增加挑战),其他语⾔⾃由选择。前端推荐React/Vue,但不强制。

Q2:可以使⽤开源项⽬改造吗?

A:可以,但必须:

• 在README中说明基于哪个开源项⽬

• 在ai_usage.md中说明你的改动

• 核⼼功能(Prompt优化、评测等)必须是你⾃⼰做的

• 不能只是简单配置,必须有实质性开发

Q3:评测数据集怎么准备?

A:

• ⾄少30条测试⽤例

• 可以是真实业务问题,也可以是构造的

• 需要包含:问题、预期答案/⾏为、难度等级

• 建议参考案例中的test_cases.json格式

Q4:Prompt迭代⼀定要3轮吗?

A:⾄少3轮,可以更多。每轮必须:

• 记录完整的Prompt⽂本

• 说明改动原因

• 提供量化的效果对⽐(不能只说"更好了")

Q5:必须做RAG吗?可以只做Agent吗?

A:RAG和Agent⼆选⼀即可,也可以两个都做(会有加分)。但必须做到:

• RAG:实现向量检索+引⽤标注

• Agent:⾄少2个⼯具+清晰的决策链路

Q6:部署到哪⾥?公司提供虚拟机吗?

A:

• 公司会提供虚拟机资源(需要⾃⼰申请)

• 也可以⽤⾃⼰的云服务器

• 或使⽤免费的云平台(Vercel、Railway等)

• 关键是提交时必须能访问

Q7:Demo视频要包含什么内容?

A:3-5分钟视频,必须包含:

1. 功能演⽰(正常使⽤流程)

2. 失败案例(如何处理异常)

3. 优化对⽐(展⽰迭代前后效果差异)

4. 可选:架构讲解、亮点介绍

Q8:组队怎么分⼯?怎么评分?

A:

• 2-3⼈组队,在README中明确分⼯

• 每⼈必须独⽴负责⾄少1个核⼼模块

• 评审时可能单独询问每个成员

• 组队作品标准不降低,但可以做得更完善

Q9:加分项是必须做的吗?

A:不是必须。加分项是在100分基础上的额外加分,做得好可以到130分。建议根据时间和能⼒选择1-2

个加分项深⼊做。

Q10:如果作业做不完怎么办?

A:

• 优先保证基础功能完整(80分线)

• 宁可功能少但做精,不要功能多但都不完整

• 如果实在做不完,可以在README中说明未完成部分和原因

• 11⽉21⽇是硬性截⽌,不接受补交

让我们⼀起通过实践成为真正的AINative开发者!🚀

有任何问题,随时在群⾥提问,我们会持续更新FAQ⽂档。

预祝⼤家都能做出优秀的作品!💪

