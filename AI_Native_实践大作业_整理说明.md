# AI Native 实践大作业整理说明

> 根据《AI Native 实践大作业说明.pdf》整理而成，聚焦具体要求与评分重点，便于快速对齐实现与提交。

## 评分要点速览（强烈建议先看）
- 基础分共 100 分：
  - 基础架构 20（前后端完整8、部署4、API6、启动2）
  - 大模型应用 25（Prompt迭代10、模型调用5、参数调优5、异常处理5）
  - RAG/Agent 25（向量检索10、引用标注5、文档处理5、知识管理5）
  - 评测优化 20（测试集5、自动化5、量化指标5、迭代证明5）
  - 工程质量 10（代码规范2、日志3、安全3、Git提交2）
- 加分项最高 +30 分：创新与优化、业务价值、AI Coding 沉淀（需充分证据）
- 扣分项：无法启动 -10、抄袭 -30、提交不规范 -10 等
- 组队评分：个人最终得分 = 项目得分 × 个人贡献系数（第1名0.95–1.00，第2名0.85–0.95，第3名0.70–0.85）
- 分档标准：优秀≥90，良好70–89，及格60–69，不及格<60
- 评审流程 15 分钟：一键启动、功能验证、评测运行、文档检查（各环节均有扣分点）

---

## 一、总体设计
- 核心目标：掌握大模型 API 调用与调参、Prompt 工程、RAG/Agent 实践、量化评测与优化、AI Coding 工具深度使用。
- 设计原则：
  - 角色公平：前端、后端、测试均需完成前后端开发。
  - 技能全覆盖：必须涉及大模型、Prompt、RAG/Agent、评测四大模块。
  - 实操优先：做出能跑、能用、能部署的真实应用。
  - 可评测：标准化提交，便于快速 review 和防抄袭。
- 作业形式：
  - 个人或小组（1–3人），周期 6 周（10/13 → 11/21）。
  - 场景选择完全自定义，结合真实业务；难度类似毕业设计，聚焦 AI 能力而非工程复杂度。

## 二、场景与功能要求
- 核心技术要求：
  - 至少调用 1 个大模型 API（Claude/GPT/国产模型均可）。
  - 必须有 Prompt 设计与优化（至少 3 轮迭代）。
  - 必须实现 RAG 或 Agent（或两者都有）。
  - 必须有量化评测体系（至少 30 条测试用例）。
- 功能要求：
  - 有可交互的 Web 前端界面（不能只是命令行）。
  - 有后端 API 服务（至少 3 个接口）。
  - 能部署到服务器并提供访问地址。
  - 解决一个真实的业务或学习场景问题。

## 三、提交要求
- 必须提交的材料：
  - `Git` 仓库地址（含渐进式提交历史，至少 10 次 commit）。
  - 部署访问地址（虚拟机 IP:端口或在线服务 URL）。
  - `README.md`（启动方式、技术架构、核心功能）。
  - 评测脚本（可独立运行，输出量化指标）。
  - Demo 视频（3–5 分钟，展示功能与优化对比）。
  - `ai_usage.md`（AI 使用记录，防抄袭关键文档）。
- 目录组织（建议）：
  - `README.md`、`ai_usage.md`、`evaluate.sh`、`.env.example`、`data/test_cases.json`。
- 文档模板要点：
  - `README.md`：快速开始、评测运行、技术架构、核心功能与性能指标、团队分工（若为小组）。
  - `ai_usage.md`：记录使用的 AI 工具、关键代码片段、AI 产出与人工修改、Prompt 的 3 个版本及效果对比、时间与效率提升、问题与解决。
- 提交方式与时间：
  - 在指定的飞书/腾讯文档表格中提交项目名称、`Git` 地址、部署地址、视频链接、团队成员等。
  - 截止时间：11 月 21 日 23:59（提交后不可修改，确保链接可访问）。

## 四、评审流程与评分标准（重点）
- 快速评审流程（每份 15 分钟）：
  - 第 1 步 环境启动（3 分钟）：一键启动、`git log` 检查、环境变量配置、服务健康检查。
    - 检查点：能否一键启动（否则 -10）、`Git` 提交是否渐进（少于 10 次扣分）。
  - 第 2 步 功能验证（5 分钟）：基础功能、引用标注/工具调用轨迹、失败案例与错误处理。
  - 第 3 步 评测验证（3 分钟）：能正常运行评测脚本，指标达基线（准确率≥60%、延迟≤5s）、≥2轮以上优化对比。
  - 第 4 步 文档检查（4 分钟）：`ai_usage.md` 详实、Prompt 迭代有数据支撑、架构图清晰、Demo 视频覆盖关键场景。

- 评分细则（基础分 100）：
  - 基础架构 20：前后端完整 8、部署 4、API 6、启动 2。
  - 大模型应用 25：Prompt 迭代 10、模型调用 5、参数调优 5、异常处理 5。
  - RAG/Agent 25：向量检索 10、引用标注 5、文档处理 5、知识管理 5。
  - 评测优化 20：测试集 5、自动化 5、量化指标 5、迭代证明 5。
  - 工程质量 10：代码规范 2、日志 3、安全 3、Git 提交 2。

- 加分项（最高 +30 分，需提供充分证据：代码/数据/截图/视频）：
  - 创新与优化类：
    - 自动化 Prompt 调优（A/B 测试或参数网格搜索，且有量化对比）：+5。
    - 多模型对比融合（≥2 模型，智能选择/融合）：+5。
    - Agent 多步协作（≥3 步工具调用，展示决策链路）：+5。
    - RAG 高级优化（混合检索、重排序、查询改写等）：+5。
  - 业务价值类：
    - 实际上线使用（团队内真实使用 ≥2 周，收集 ≥20 条用户反馈并迭代优化）：+10。
  - AI Coding 沉淀类：
    - 深度使用记录（`ai_usage.md` >1500 字，≥3 个具体案例，含提效数据）：+5。
    - Prompt 模式库（总结 ≥3 个可复用的 Prompt 设计模式，每个有验证数据）：+5。
  - 重要说明：不能只有文档说明，必须有实际实现；UI 美观不作为主要评分依据。

- 扣分项（示例）：
  - 无法启动 -10；抄袭 -30；提交不规范 -10；一次性提交或集中在最后 1–2 天 -10；`ai_usage.md` 空洞 -10；缺少中文注释 -5。

- 分档标准：
  - 优秀（≥90）：基础要求完成度高、至少 2–3 个加分项、工程质量与文档规范、具备创新或深度思考。
  - 良好（70–89）：满足所有基础要求，部分有亮点。
  - 及格（60–69）：基本满足要求，但有明显不足。
  - 不及格（<60）：无法启动或功能严重缺失、没有评测体系、Prompt 迭代缺失或造假、明显抄袭。

- 组队评分与贡献系数：
  - 公式：个人最终得分 = 项目得分 × 个人贡献系数（项目得分=基础分+加分-扣分）。
  - 系数设定：第1名 0.95–1.00（核心模块）、第2名 0.85–0.95（重要模块）、第3名 0.70–0.85（支撑工作）。
  - 约束：组内第1名最高 1.0；系数总和≈组内人数×(0.85–0.90)。
  - 排名依据权重：Git 提交数据 40%、模块复杂度 35%、答辩表现 25%。
  - 示例：3人组队项目得分 120 → 第1名 120、 第2名 105.6、 第3名 90；单人同样项目直接 120。

## 五、防抄袭与作弊机制
- 过程留痕检查：
  - `Git` 提交历史：至少 10 次渐进式提交；一次性提交扣 20；集中在最后 1–2 天扣 10。
  - `ai_usage.md`：必须详细具体、含关键代码片段与问题解决；空洞或过于简单扣 10；缺少中文注释扣 5。
- 个性化要求：
  - 在系统 Prompt 中加入工号后 4 位；测试数据主题个性化；评测脚本输出包含提交人 ID。
- 人工抽查：
  - 代码讲解与现场小改动（如调整 `top-k`、新增 Prompt 约束）验证真实完成；答不上扣 30–50。
- 合作规范：
  - 允许讨论方案、共享公开资料、互评建议；严禁复制核心代码/Prompt、提交相同数据集、一人多名。

## 六、FAQ 关键答疑
- 技术栈：后端禁用 Java（增加挑战），其他语言自由；前端推荐 React/Vue。
- 评测数据集：≥30 条；含问题、预期答案/行为、难度等级；建议使用 `data/test_cases.json` 格式。
- RAG 或 Agent：二选一或都做；RAG 需向量检索+引用标注；Agent 需 ≥2 工具 + 清晰决策链路。
- 部署：公司虚拟机或自有/免费云平台（如 Vercel/Railway）；提交时必须可访问。
- Demo 视频：3–5 分钟，覆盖功能演示、失败案例、优化对比；可选架构与亮点。
- 不足补救：优先保证基础功能完整（80分线）；未完成部分在 `README` 中说明原因；11/21 为硬性截止。

---

## 附：评测运行与提交清单（自查）
- 环境与启动：`.env` 可配置、一键启动成功、健康检查 200。
- 功能与交互：基础功能完整、异常处理优雅、引用标注/工具轨迹可见。
- 评测与优化：脚本可跑、指标达基线（准确率≥60%、延迟≤5s）、至少 2 轮优化对比。
- 文档与演示：`README`/架构图/`ai_usage.md` 规范完备、Demo 视频覆盖关键场景。
- Git 记录：≥10 次渐进式提交、时间分散、提交信息规范。
- 加分项证据：数据、截图、视频齐全；上线反馈记录完整（若申报）。

> 建议将本说明作为项目 `docs/评分对照表.md` 放入仓库，并在开发全周期对照自查，以确保得分最大化。